# The following properties are taken from the v2.2 documentation
# for the Kafka Producer and can be uncommented and set as required.
# NOTE key.serializer and value.serializer should not be set as
# these are set within stroom.
# https://kafka.apache.org/22/documentation.html#producerconfigs

# The number of acknowledgments the producer requires the leader to have
# received before considering a request complete. This controls the durability
# of records that are sent. The following settings are allowed:acks=0 If set to
# zero then the producer will not wait for any acknowledgment from the server
# at all. The record will be immediately added to the socket buffer and
# considered sent. No guarantee can be made that the server has received the
# record in this case, and the retries configuration will not take effect (as
# the client won't generally know of any failures). The offset given back for
# each record will always be set to -1.acks=1 This will mean the leader will
# write the record to its local log but will respond without awaiting full
# acknowledgement from all followers. In this case should the leader fail
# immediately after acknowledging the record but before the followers have
# replicated it then the record will be lost.acks=all This means the leader
# will wait for the full set of in-sync replicas to acknowledge the record.
# This guarantees that the record will not be lost as long as at least one
# in-sync replica remains alive. This is the strongest available guarantee.
# This is equivalent to the acks=-1 setting.
# Type: string
# Default value: 1
# Valid values: [all, -1, 0, 1]
#acks=

# A list of host/port pairs to use for establishing the initial connection to
# the Kafka cluster. The client will make use of all servers irrespective of
# which servers are specified here for bootstrapping this list only impacts the
# initial hosts used to discover the full set of servers. This list should be
# in the form host1:port1,host2:port2,.... Since these servers are just used
# for the initial connection to discover the full cluster membership (which may
# change dynamically), this list need not contain the full set of servers (you
# may want more than one, though, in case a server is down).non-null string
# Type: list
# Default value: ""
# Valid values: non-null string
#bootstrap.servers=

# The total bytes of memory the producer can use to buffer records waiting to
# be sent to the server. If records are sent faster than they can be delivered
# to the server the producer will block for max.block.ms after which it will
# throw an exception.This setting should correspond roughly to the total memory
# the producer will use, but is not a hard bound since not all memory the
# producer uses is used for buffering. Some additional memory will be used for
# compression (if compression is enabled) as well as for maintaining in-flight
# requests.[0,...]
# Type: long
# Default value: 33554432
# Valid values: [0,...]
#buffer.memory=

# The compression type for all data generated by the producer. The default is
# none (i.e. no compression). Valid values are none, gzip, snappy, lz4,
# or zstd. Compression is of full batches of data, so the efficacy of batching
# will also impact the compression ratio (more batching means better
# compression).
# Type: string
# Default value: none
# Valid values:
#compression.type=

# Setting a value greater than zero will cause the client to resend any record
# whose send fails with a potentially transient error. Note that this retry is
# no different than if the client resent the record upon receiving the error.
# Allowing retries without setting max.in.flight.requests.per.connection to 1
# will potentially change the ordering of records because if two batches are
# sent to a single partition, and the first fails and is retried but the second
# succeeds, then the records in the second batch may appear first. Note
# additionall that produce requests will be failed before the number of retries
# has been exhausted if the timeout configured by delivery.timeout.ms expires
# first before successful acknowledgement. Users should generally prefer to
# leave this config unset and instead use delivery.timeout.ms to control retry
# behavior.[0,...,2147483647]
# Type: int
# Default value: 2147483647
# Valid values: [0,...,2147483647]
#retries=

# The password of the private key in the key store file. This is optional for
# client.
# Type: password
# Default value: null
# Valid values:
#ssl.key.password=

# The location of the key store file. This is optional for client and can be
# used for two-way authentication for client.
# Type: string
# Default value: null
# Valid values:
#ssl.keystore.location=

# The store password for the key store file. This is optional for client and
# only needed if ssl.keystore.location is configured.
# Type: password
# Default value: null
# Valid values:
#ssl.keystore.password=

# The location of the trust store file.
# Type: string
# Default value: null
# Valid values:
#ssl.truststore.location=

# The password for the trust store file. If a password is not set access to the
# truststore is still available, but integrity checking is disabled.
# Type: password
# Default value: null
# Valid values:
#ssl.truststore.password=

# The producer will attempt to batch records together into fewer requests
# whenever multiple records are being sent to the same partition. This helps
# performance on both the client and the server. This configuration controls
# the default batch size in bytes.No attempt will be made to batch records
# larger than this size.Requests sent to brokers will contain multiple batches,
# one for each partition with data available to be sent.A small batch size will
# make batching less common and may reduce throughput (a batch size of zero
# will disable batching entirely). A very large batch size may use memory a bit
# more wastefully as we will always allocate a buffer of the specified batch
# size in anticipation of additional records.[0,...]
# Type: int
# Default value: 16384
# Valid values: [0,...]
#batch.size=

# Controls how the client uses DNS lookups.If set to use_all_dns_ips then, when
# the lookup returns multiple IP addresses for a hostname, they will all be
# attempted to connect to before failing the connection. Applies to both
# bootstrap and advertised servers.If the value
# is resolve_canonical_bootstrap_servers_only each entry will be resolved and
# expanded into a list of canonical names.[default, use_all_dns_ips,
# resolve_canonical_bootstrap_servers_only]
# Type: string
# Default value: default
# Valid values: [default, use_all_dns_ips, resolve_canonical_bootstrap_servers_only]
#client.dns.lookup=

# An id string to pass to the server when making requests. The purpose of this
# is to be able to track the source of requests beyond just ip/port by allowing
# a logical application name to be included in server-side request logging.
# Type: string
# Default value: ""
# Valid values:
#client.id=

# Close idle connections after the number of milliseconds specified by this
# config.
# Type: long
# Default value: 540000
# Valid values:
#connections.max.idle.ms=

# An upper bound on the time to report success or failure after a call
# to send() returns. This limits the total time that a record will be delayed
# prior to sending, the time to await acknowledgement from the broker (if
# expected), and the time allowed for retriable send failures. The producer may
# report failure to send a record earlier than this config if either an
# unrecoverable error is encountered, the retries have been exhausted, or the
# record is added to a batch which reached an earlier delivery expiration
# deadline. The value of this config should be greater than or equal to the sum
# of request.timeout.ms and linger.ms.[0,...]
# Type: int
# Default value: 120000
# Valid values: [0,...]
#delivery.timeout.ms=

# The producer groups together any records that arrive in between request
# transmissions into a single batched request. Normally this occurs only under
# load when records arrive faster than they can be sent out. However in some
# circumstances the client may want to reduce the number of requests even under
# moderate load. This setting accomplishes this by adding a small amount of
# artificial delayâthat is, rather than immediately sending out a record the
# producer will wait for up to the given delay to allow other records to be
# sent so that the sends can be batched together. This can be thought of as
# analogous to Nagle's algorithm in TCP. This setting gives the upper bound on
# the delay for batching: once we get batch.size worth of records for a
# partition it will be sent immediately regardless of this setting, however if
# we have fewer than this many bytes accumulated for this partition we will
# 'linger' for the specified time waiting for more records to show up. This
# setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example,
# would have the effect of reducing the number of requests sent but would add
# up to 5ms of latency to records sent in the absence of load.[0,...]
# Type: int
# Default value: 0
# Valid values: [0,...]
#linger.ms=

# The configuration controls how
# long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.These
# methods can be blocked either because the buffer is full or metadata
# unavailable.Blocking in the user-supplied serializers or partitioner will not
# be counted against this timeout.[0,...]
# Type: long
# Default value: 60000
# Valid values: [0,...]
#max.block.ms=

# The maximum size of a request in bytes. This setting will limit the number of
# record batches the producer will send in a single request to avoid sending
# huge requests. This is also effectively a cap on the maximum record batch
# size. Note that the server has its own cap on record batch size which may be
# different from this.[0,...]
# Type: int
# Default value: 1048576
# Valid values: [0,...]
#max.request.size=

# Partitioner class that implements
# the org.apache.kafka.clients.producer.Partitioner interface.
# Type: class
# Default value: org.apache.kafka.clients.producer.internals.DefaultPartitioner
# Valid values:
#partitioner.class=

# The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If
# the value is -1, the OS default will be used.[-1,...]
# Type: int
# Default value: 32768
# Valid values: [-1,...]
#receive.buffer.bytes=

# The configuration controls the maximum amount of time the client will wait
# for the response of a request. If the response is not received before the
# timeout elapses the client will resend the request if necessary or fail the
# request if retries are exhausted. This should be larger
# than replica.lag.time.max.ms (a broker configuration) to reduce the
# possibility of message duplication due to unnecessary producer
# retries.[0,...]
# Type: int
# Default value: 30000
# Valid values: [0,...]
#request.timeout.ms=

# The fully qualified name of a SASL client callback handler class that
# implements the AuthenticateCallbackHandler interface.
# Type: class
# Default value: null
# Valid values:
#sasl.client.callback.handler.class=

# JAAS login context parameters for SASL connections in the format used by JAAS
# configuration files. JAAS configuration file format is described here. The
# format for the value is: 'loginModuleClass controlFlag
# (optionName=optionValue)*;'. For brokers, the config must be prefixed with
# listener prefix and SASL mechanism name in lower-case. For example,
# listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=com.example.ScramLoginModule
# required;
# Type: password
# Default value: null
# Valid values:
#sasl.jaas.config=

# The Kerberos principal name that Kafka runs as. This can be defined either in
# Kafka's JAAS config or in Kafka's config.
# Type: string
# Default value: null
# Valid values:
#sasl.kerberos.service.name=

# The fully qualified name of a SASL login callback handler class that
# implements the AuthenticateCallbackHandler interface. For brokers, login
# callback handler config must be prefixed with listener prefix and SASL
# mechanism name in lower-case. For example,
# listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler
# Type: class
# Default value: null
# Valid values:
#sasl.login.callback.handler.class=

# The fully qualified name of a class that implements the Login interface. For
# brokers, login config must be prefixed with listener prefix and SASL
# mechanism name in lower-case. For example,
# listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin
# Type: class
# Default value: null
# Valid values:
#sasl.login.class=

# SASL mechanism used for client connections. This may be any mechanism for
# which a security provider is available. GSSAPI is the default mechanism.
# Type: string
# Default value: GSSAPI
# Valid values:
#sasl.mechanism=

# Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL,
# SASL_PLAINTEXT, SASL_SSL.
# Type: string
# Default value: PLAINTEXT
# Valid values:
#security.protocol=

# The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the
# value is -1, the OS default will be used.[-1,...]
# Type: int
# Default value: 131072
# Valid values: [-1,...]
#send.buffer.bytes=

# The list of protocols enabled for SSL connections.
# Type: list
# Default value: TLSv1.2,TLSv1.1,TLSv1
# Valid values:
#ssl.enabled.protocols=

# The file format of the key store file. This is optional for client.
# Type: string
# Default value: JKS
# Valid values:
#ssl.keystore.type=

# The SSL protocol used to generate the SSLContext. Default setting is TLS,
# which is fine for most cases. Allowed values in recent JVMs are TLS, TLSv1.1
# and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in older JVMs, but their
# usage is discouraged due to known security vulnerabilities.
# Type: string
# Default value: TLS
# Valid values:
#ssl.protocol=

# The name of the security provider used for SSL connections. Default value is
# the default security provider of the JVM.
# Type: string
# Default value: null
# Valid values:
#ssl.provider=

# The file format of the trust store file.
# Type: string
# Default value: JKS
# Valid values:
#ssl.truststore.type=

# When set to 'true', the producer will ensure that exactly one copy of each
# message is written in the stream. If 'false', producer retries due to broker
# failures, etc., may write duplicates of the retried message in the stream.
# Note that enabling idempotence
# requires max.in.flight.requests.per.connection to be less than or equal to
# 5, retries to be greater than 0 and acks must be 'all'. If these values are
# not explicitly set by the user, suitable values will be chosen. If
# incompatible values are set, a ConfigException will be thrown.
# Type: boolean
# Default value: false
# Valid values:
#enable.idempotence=

# A list of classes to use as interceptors. Implementing
# the org.apache.kafka.clients.producer.ProducerInterceptor interface allows
# you to intercept (and possibly mutate) the records received by the producer
# before they are published to the Kafka cluster. By default, there are no
# interceptors.non-null string
# Type: list
# Default value: ""
# Valid values: non-null string
#interceptor.classes=

# The maximum number of unacknowledged requests the client will send on a
# single connection before blocking. Note that if this setting is set to be
# greater than 1 and there are failed sends, there is a risk of message
# re-ordering due to retries (i.e., if retries are enabled).[1,...]
# Type: int
# Default value: 5
# Valid values: [1,...]
#max.in.flight.requests.per.connection=

# The period of time in milliseconds after which we force a refresh of metadata
# even if we haven't seen any partition leadership changes to proactively
# discover any new brokers or partitions.[0,...]
# Type: long
# Default value: 300000
# Valid values: [0,...]
#metadata.max.age.ms=

# A list of classes to use as metrics reporters. Implementing
# the org.apache.kafka.common.metrics.MetricsReporter interface allows plugging
# in classes that will be notified of new metric creation. The JmxReporter is
# always included to register JMX statistics.non-null string
# Type: list
# Default value: ""
# Valid values: non-null string
#metric.reporters=

# The number of samples maintained to compute metrics.[1,...]
# Type: int
# Default value: 2
# Valid values: [1,...]
#metrics.num.samples=

# The highest recording level for metrics.[INFO, DEBUG]
# Type: string
# Default value: INFO
# Valid values: [INFO, DEBUG]
#metrics.recording.level=

# The window of time a metrics sample is computed over.[0,...]
# Type: long
# Default value: 30000
# Valid values: [0,...]
#metrics.sample.window.ms=

# The maximum amount of time in milliseconds to wait when reconnecting to a
# broker that has repeatedly failed to connect. If provided, the backoff per
# host will increase exponentially for each consecutive connection failure, up
# to this maximum. After calculating the backoff increase, 20% random jitter is
# added to avoid connection storms.[0,...]
# Type: long
# Default value: 1000
# Valid values: [0,...]
#reconnect.backoff.max.ms=

# The base amount of time to wait before attempting to reconnect to a given
# host. This avoids repeatedly connecting to a host in a tight loop. This
# backoff applies to all connection attempts by the client to a broker.[0,...]
# Type: long
# Default value: 50
# Valid values: [0,...]
#reconnect.backoff.ms=

# The amount of time to wait before attempting to retry a failed request to a
# given topic partition. This avoids repeatedly sending requests in a tight
# loop under some failure scenarios.[0,...]
# Type: long
# Default value: 100
# Valid values: [0,...]
#retry.backoff.ms=

# Kerberos kinit command path.
# Type: string
# Default value: /usr/bin/kinit
# Valid values:
#sasl.kerberos.kinit.cmd=

# Login thread sleep time between refresh attempts.
# Type: long
# Default value: 60000
# Valid values:
#sasl.kerberos.min.time.before.relogin=

# Percentage of random jitter added to the renewal time.
# Type: double
# Default value: 0.05
# Valid values:
#sasl.kerberos.ticket.renew.jitter=

# Login thread will sleep until the specified window factor of time from last
# refresh to ticket's expiry has been reached, at which time it will try to
# renew the ticket.
# Type: double
# Default value: 0.8
# Valid values:
#sasl.kerberos.ticket.renew.window.factor=

# The amount of buffer time before credential expiration to maintain when
# refreshing a credential, in seconds. If a refresh would otherwise occur
# closer to expiration than the number of buffer seconds then the refresh will
# be moved up to maintain as much of the buffer time as possible. Legal values
# are between 0 and 3600 (1 hour); a default value of 300 (5 minutes) is used
# if no value is specified. This value and
# sasl.login.refresh.min.period.seconds are both ignored if their sum exceeds
# the remaining lifetime of a credential. Currently applies only to
# OAUTHBEARER.[0,...,3600]
# Type: short
# Default value: 300
# Valid values: [0,...,3600]
#sasl.login.refresh.buffer.seconds=

# The desired minimum time for the login refresh thread to wait before
# refreshing a credential, in seconds. Legal values are between 0 and 900 (15
# minutes); a default value of 60 (1 minute) is used if no value is specified.
# This value and sasl.login.refresh.buffer.seconds are both ignored if their
# sum exceeds the remaining lifetime of a credential. Currently applies only to
# OAUTHBEARER.[0,...,900]
# Type: short
# Default value: 60
# Valid values: [0,...,900]
#sasl.login.refresh.min.period.seconds=

# Login refresh thread will sleep until the specified window factor relative to
# the credential's lifetime has been reached, at which time it will try to
# refresh the credential. Legal values are between 0.5 (50%) and 1.0 (100%)
# inclusive; a default value of 0.8 (80%) is used if no value is specified.
# Currently applies only to OAUTHBEARER.[0.5,...,1.0]
# Type: double
# Default value: 0.8
# Valid values: [0.5,...,1.0]
#sasl.login.refresh.window.factor=

# The maximum amount of random jitter relative to the credential's lifetime
# that is added to the login refresh thread's sleep time. Legal values are
# between 0 and 0.25 (25%) inclusive; a default value of 0.05 (5%) is used if
# no value is specified. Currently applies only to OAUTHBEARER.[0.0,...,0.25]
# Type: double
# Default value: 0.05
# Valid values: [0.0,...,0.25]
#sasl.login.refresh.window.jitter=

# A list of cipher suites. This is a named combination of authentication,
# encryption, MAC and key exchange algorithm used to negotiate the security
# settings for a network connection using TLS or SSL network protocol. By
# default all the available cipher suites are supported.
# Type: list
# Default value: null
# Valid values:
#ssl.cipher.suites=

# The endpoint identification algorithm to validate server hostname using
# server certificate.
# Type: string
# Default value: https
# Valid values:
#ssl.endpoint.identification.algorithm=

# The algorithm used by key manager factory for SSL connections. Default value
# is the key manager factory algorithm configured for the Java Virtual Machine.
# Type: string
# Default value: SunX509
# Valid values:
#ssl.keymanager.algorithm=

# The SecureRandom PRNG implementation to use for SSL cryptography operations.
# Type: string
# Default value: null
# Valid values:
#ssl.secure.random.implementation=

# The algorithm used by trust manager factory for SSL connections. Default
# value is the trust manager factory algorithm configured for the Java Virtual
# Machine.
# Type: string
# Default value: PKIX
# Valid values:
#ssl.trustmanager.algorithm=

# The maximum amount of time in ms that the transaction coordinator will wait
# for a transaction status update from the producer before proactively aborting
# the ongoing transaction.If this value is larger than the
# transaction.max.timeout.ms setting in the broker, the request will fail with
# a InvalidTransactionTimeout error.
# Type: int
# Default value: 60000
# Valid values:
#transaction.timeout.ms=

# The TransactionalId to use for transactional delivery. This enables
# reliability semantics which span multiple producer sessions since it allows
# the client to guarantee that transactions using the same TransactionalId have
# been completed prior to starting any new transactions. If no TransactionalId
# is provided, then the producer is limited to idempotent delivery. Note
# that enable.idempotence must be enabled if a TransactionalId is configured.
# The default is null, which means transactions cannot be used. Note that, by
# default, transactions require a cluster of at least three brokers which is
# the recommended setting for production; for development you can change this,
# by adjusting broker
# setting transaction.state.log.replication.factor.non-empty string
# Type: string
# Default value: null
# Valid values: non-empty string
#transactional.id=

